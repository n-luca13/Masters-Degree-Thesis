<resources xmlns:tools="http://schemas.android.com/tools">
    <string name="app_name">Applicazione Tesi</string>
    <string name="u_annulla_u"><u>Annulla</u></string>
    <string name="tocca_per_parlare">Tocca l\'icona Microfono per parlare</string>
    <string name="tocca_per_interrompere">Tocca per interrompere</string>
    <string name="placeholder_text" tools:ignore="Typos">"Con l’emergere dei tanto citati Big Data, alcuni autori hanno sostenuto che la disponibilità di una grande mole di dati, combinata con le accresciute capacità computazionali e le opportune tecniche statistiche, consenta di accantonare il metodo scientifico. Di qui il titolo del noto articolo del 2008 di Chris Anderson, La fine della teoria.
\nSecondo Anderson, è ormai controproducente fondare la selezione delle variabili su ipotesi attorno al funzionamento dei fenomeni; non dobbiamo più formulare dei modelli da mettere alla prova con degli esperimenti. Gli algoritmi di machine learning consentono di testare tutte le possibili correlazioni; e in assenza di ipotesi, promettono una rappresentazione completa, obiettiva e imparziale dei fenomeni osservati; offrono un metodo super partes, automatico e fondato sulla ricerca di correlazioni. Vi è dunque la tentazione di usare tali sistemi per giustificare dei processi decisionali, con il rischio di riprodurre pregiudizi e iniquità.
\nA distanza di più di dieci anni dalla sua pubblicazione, l’articolo di Anderson è oggi talvolta interpretato come una sorta di monito per i ricercatori su cosa non fare quando si fa ricerca. La raccolta dei dati non può prescindere da una teoria sui fenomeni osservati, così come l’attribuzione di significato alle correlazioni che emergono dalle analisi di data mining dipende da un’interpretazione non può sparire di punto in bianco. Eleonora Priori parla infatti del data scientist come di un «narratore» o «storyteller dei dati», sottolineando come i risultati dell’analisi siano «fortemente influenzati dalla lettura che si dà di quegli stessi dati.
\nE numerose scelte in fase di progettazione portano a «valorizzare aspetti assai diversi» . Ogni implementazione costituisce un’opera di astrazione che lascia dietro di sé un «residuo, un insieme di informazioni scartate» . I modelli computazionali che ne risultano non sono neutrali. Si pensi alla scelta di quali algoritmi e tecniche statistiche adoperare, dei dati utilizzare e del peso di ciascuna variabile – oppure alla scelta del parametro k nell’algoritmo k-nearest neighbour. Se, dunque, anche questi modelli si fondano su assunti teorici o mentali, è fondamentale che questi siano resi noti, affinché si possa aprire la scatola nera e rivelare eventuali errori e pregiudizi codificati.
\nAbbiamo visto che le tecniche di raccomandazione sono molteplici e assumono diverse sfumature; inoltre, va detto che sono stati implementati nel tempo vari meccanismi (come raccomandazioni aleatorie) e sono state aggiunte ulteriori informazioni (come nel content-based filtering). In ogni caso, però, la logica di fondo non cambia; e le ipotesi di partenza sono sostanzialmente le stesse. Gli strumenti predittivi confidano nella regolarità dei nostri comportamenti, partendo dal presupposto che il futuro non sarà così diverso dal passato. E così facendo incentivano comportamenti regolari e prevedibili e – al contempo – rimettono interamente al calcolato la responsabilità di questi calcoli, di queste conseguenze.
\nQuesto discorso si estende ad altri sistemi di machine learning adoperati in diversi processi decisionali – per prevedere il crimine, il tasso di recidiva dei detenuti o la propensione di un candidato a un posto di lavoro: anche questi sollecitano la reiterazione del passato, e rischiano così di riprodurre i pregiudizi della società, riducendo ulteriormente le opportunità dei più deboli. Gli algoritmi predittivi sono essenzialmente conservatori. Ad esempio, nel derivare il rischio di recidiva da caratteristiche dell’ambiente sociale da cui proviene il detenuto e dal comportamento di individui a lui affini, si accresce la probabilità che quanti provengono da ambienti più sfavorevoli vadano incontro a trattamenti altrettanto severi.
\nLe implementazioni dell’apprendimento automatico promettono nel complesso una  maggiore efficienza; tuttavia, l’eventuale presenza di errori e pregiudizi codificati, un’errata interpretazione delle statistiche, l’utilizzo di dati vicarianti e la mancanza di opportuni cicli di feedback, possono provocare dei danni collaterali. Così, se i dataset o le regole codificate sono viziati da errori, gli algoritmi pervengono a risultati distorti: se inserisci spazzatura, esce spazzatura.
\nE quando l’algoritmo fallisce occorre affrontare il nodo della responsabilità: se l’auto autonoma investe un passante, non sarà certo il software a dover comparire in tribunale per rispondere delle sue azioni. Un’IA non può essere responsabile di azioni immorali, non può distinguere il bene dal male, e non fa altro che eseguire delle istruzioni.
\nDopotutto, le numerose incarnazioni dell’apprendimento non hanno molto a che fare con l’intelligenza umana. Anzi, sotto un certo punto di vista si tratta di macchine abbastanza stupide: se un bambino sfiora la fiamma di un fornello da cucina non sente la necessità di ripetere l’operazione un migliaio di volte per appurare di avere a che fare con una forte correlazione, bensì gli basta quell’occasione per associare una causa a un determinato effetto. E piuttosto che di IA potremmo parlare di intelligenza statistica, a sottolineare il fatto che questi algoritmi hanno successo solo grazie a tecniche statistiche applicate a vasti dataset e in fattispecie ben circoscritte, con problematiche ben definite.
\nDi qui l’affermazione di Luciano Floridi secondo cui l’IA odierna rappresenta un «divorzio fra l’agency, intesa come capacità di agire per perseguire con successo un obiettivo, e ogni [forma di] intelligenza che deve essere esercitata nel farlo» . L’approccio imitativo dell’intelligenza umana che ha contrassegnato gli albori dell’IA si è rivelato fallimentare, mentre quello «ingegneristico» – entro cui l’importante è trovare una soluzione razionale a un problema, non come la si raggiunge – ha avuto un successo straordinario , determinando il proliferare di sistemi capaci di agire con successo rispetto a un fine pur senza disporre di alcuna intelligenza. Questi modelli sono poi efficienti in ambienti strutturati e ripetitivi, con azioni semplici soprattutto in termini di abilità e destrezza. Così, sempre più spesso gli ambienti sono riformulati per garantire il successo delle IA (pensiamo a corsie e quartieri dedicati alle auto autonome).
\nDunque sono molti i rischi derivanti da un uso improprio di questa intelligenza statistica. E la morale è che occorre conoscere gli algoritmi e i loro effetti.
\nAnzitutto, dobbiamo scegliere con cura quali compiti assoggettare al calcolo automatico, e assicurarci che – una volta operativi – questi sistemi realizzino esattamente quello che vogliamo (e non semplicemente quello che gli abbiamo richiesto di fare).
\nOccorre poi verificare l’accuratezza e l’equità dei risultati, essere consapevoli delle problematiche dell’equità e dei pregiudizi codificati; comprendere i limiti dei dati e – se possibile – cercare di rimuovere o limitare i bias – ad esempio utilizzando tecniche di oversampling per aumentare il peso di classi minoritarie.
\nDobbiamo rendere i modelli interpretabili o almeno fornire una spiegazione dei risultati. E dobbiamo ottimizzarli affinché rispondano ad alcuni valori e principi piuttosto che altri; lo stesso concetto di equità implica diversi criteri di misura che possono collidere tra loro: vogliamo realizzare un equità in termini di risultati o di opportunità? Un equità tra individui o tra gruppi?

\nAlla luce di quanto detto sinora, occorre vigilare sulla progettazione e sull’operato di questi sistemi, mettendo in discussione l’algoritmo, la funzione obiettivo o i dati utilizzati.
\nD’altra parte, interventi come quello di Luciano Floridi suggeriscono una necessità (sempre più prossima) di accantonare o affiancare il modello standard dell’intelligenza statistica, riavviare cioè quell’ardua impresa dell’Intelligenza Artificiale che ha caratterizzato gli albori della disciplina."</string>
    <string name="scegli_un_occorenza_di">Scegli un\'occorrenza di&#160;</string>
    <string name="in_ascolto">In ascolto</string>
    <string name="next">next</string>
    <string name="previous">previous</string>
    <string name="non_mostrare">Non mostrare più</string>
    <string name="ok">OK</string>
    <string name="taglia">Taglia</string>
    <string name="copia">Copia</string>
    <string name="rimuovi">Rimuovi</string>
    <string name="incolla">Incolla</string>
    <string name="undo_button">Undo button</string>
    <string name="clear_button">Clear button</string>
    <string name="bold_button">Bold button</string>
    <string name="highlighter_button">Highlighter button</string>
    <string name="keyboard_button">keyboard button</string>
    <string name="microphone_button">Microphone button</string>
    <string name="tutorial_button">Tutorial button</string>
    <string name="occurrence_text">%1$d/%2$d</string>
    <string name="avanti">Avanti</string>
    <string name="titolo">Titolo</string>
    <string name="app_settings">app settings</string>
    <string name="pulsante_microfono">pulsante microfono</string>
</resources>